{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5622135a",
   "metadata": {},
   "source": [
    "**The original raw dataset is over 3GB, excess GitHub limit, this only can process a subset sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ccd45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('DP_bicycle').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106120cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data which has a header. Schema is automatically configured.\n",
    "df_raw = spark.read.csv('Datasets/London.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f2151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af05140",
   "metadata": {},
   "source": [
    "*data cleaning (3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d5f6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 397075 from raw dataset, 30784706 after Clean.\n"
     ]
    }
   ],
   "source": [
    "# Row count before clean\n",
    "rowCount_before = df_raw.count()\n",
    "# Remove all rows has null value\n",
    "df_raw = df_raw.na.drop()\n",
    "# Remove extrame value\n",
    "# We only consider normal commuting to work, choose less than 2 hour (7200 seconds)\n",
    "df_raw = df_raw.filter(col(\"duration\")<=\"7200\")\n",
    "rowCount_after = df_raw.count()\n",
    "# Clean result\n",
    "print(\"Removed \" + str(rowCount_before-rowCount_after) + \" from raw dataset, \" + str(rowCount_after) + \" after Clean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a96690b",
   "metadata": {},
   "source": [
    "*Data construction (3.3)\n",
    "*Add Date, Weekday, Hour columns from start_rental_date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b767d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rental_id: integer (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- end_rental_date_time: timestamp (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- start_rental_date_time: timestamp (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- Time_Hour: integer (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw = df_raw.withColumn('date', to_date(\"start_rental_date_time\"))\\\n",
    "                           .withColumn('Time_Hour', hour(\"start_rental_date_time\"))\\\n",
    "                           .withColumn('DayOfWeek', date_format(\"start_rental_date_time\", 'EEEE'))\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8401cbf",
   "metadata": {},
   "source": [
    "*We create a new dataset (df1) including communiting transactions between St.Peter's Terrace and Parsons Green Station\n",
    "*df2 is the people start to ride from St.Peter's Terrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08a4fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_raw.filter((col(\"start_station_id\") == 729) & (col(\"end_station_id\") == 671)\\\n",
    "              | (col(\"start_station_id\") == 671) & (col(\"end_station_id\") == 729) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dc3ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.filter(col(\"start_station_id\") == 729)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c4d6a",
   "metadata": {},
   "source": [
    "hourly statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f215f458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+-----+------------------+\n",
      "|int_week|DayOfWeek|Time_Hour|count|      avg_duration|\n",
      "+--------+---------+---------+-----+------------------+\n",
      "|       3|  Tuesday|        0|    1|             240.0|\n",
      "|       3|  Tuesday|        5|   10|             172.1|\n",
      "|       3|  Tuesday|        6|  110|199.96363636363637|\n",
      "|       3|  Tuesday|        7|  488|202.53483606557376|\n",
      "|       3|  Tuesday|        8|  243|215.00411522633746|\n",
      "|       3|  Tuesday|        9|   73|198.68493150684932|\n",
      "|       3|  Tuesday|       10|   28|199.28571428571428|\n",
      "|       3|  Tuesday|       11|   29| 397.2413793103448|\n",
      "|       3|  Tuesday|       12|   25|             211.2|\n",
      "|       3|  Tuesday|       13|   17| 204.7058823529412|\n",
      "|       3|  Tuesday|       14|   12|             285.0|\n",
      "|       3|  Tuesday|       15|   22|215.45454545454547|\n",
      "|       3|  Tuesday|       16|   70|206.57142857142858|\n",
      "|       3|  Tuesday|       17|  194| 219.2577319587629|\n",
      "|       3|  Tuesday|       18|  163|233.68098159509202|\n",
      "|       3|  Tuesday|       19|   47|219.04255319148936|\n",
      "|       3|  Tuesday|       20|   23| 219.2173913043478|\n",
      "|       3|  Tuesday|       21|   10|             198.0|\n",
      "|       3|  Tuesday|       22|    4|             300.0|\n",
      "+--------+---------+---------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn(\"int_week\", dayofweek(\"start_rental_date_time\"))\\\n",
    "        .groupBy(\"int_week\",\"DayOfWeek\",\"Time_Hour\")\\\n",
    "        .agg(count(\"duration\").alias(\"count\"), avg(\"duration\").alias(\"avg_duration\"))\\\n",
    "        .filter(col(\"int_week\") ==3)\\\n",
    "        .orderBy(\"int_week\",\"Time_Hour\")\\\n",
    "        .show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628e7f9",
   "metadata": {},
   "source": [
    "*Data Integration (3.4)\n",
    "*Load weather data and join with bicycles data by column 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0357816b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datetime: date (nullable = true)\n",
      " |-- feelslike: double (nullable = true)\n",
      " |-- precip: double (nullable = true)\n",
      " |-- precipcover: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- day_length: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_weather = spark.read.csv('Datasets/London_weather.csv', header=True, inferSchema=True)\n",
    "df_weather.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8adfa",
   "metadata": {},
   "source": [
    "Convert to statistical dataset and merge with weather (3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9934bfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+------------------+----------+---------+------+-----------+---------+----------+\n",
      "|      date|DayOfWeek|count|      avg_duration|  datetime|feelslike|precip|precipcover|windspeed|day_length|\n",
      "+----------+---------+-----+------------------+----------+---------+------+-----------+---------+----------+\n",
      "|2017-01-01|   Sunday|    2|             180.0|2017-01-01|      3.8| 8.976|       12.5|     20.3|     7.936|\n",
      "|2017-01-02|   Monday|    7|171.42857142857142|2017-01-02|      0.9|   2.0|       4.17|     15.4|     7.956|\n",
      "|2017-01-03|  Tuesday|   13|203.07692307692307|2017-01-03|     -1.0| 0.001|       4.17|     20.2|     7.978|\n",
      "|2017-01-04|Wednesday|   14|188.57142857142858|2017-01-04|      2.5|   0.0|        0.0|     17.6|     8.002|\n",
      "|2017-01-05| Thursday|   14|162.85714285714286|2017-01-05|      2.2|   0.0|        0.0|      7.4|     8.027|\n",
      "|2017-01-06|   Friday|   10|             198.0|2017-01-06|      1.8| 1.002|       8.33|     16.4|     8.054|\n",
      "|2017-01-07| Saturday|    5|             168.0|2017-01-07|      7.8| 2.214|       12.5|     15.6|     8.082|\n",
      "|2017-01-08|   Sunday|    2|             330.0|2017-01-08|      8.9| 0.201|       4.17|      7.9|     8.113|\n",
      "|2017-01-09|   Monday|   14|205.71428571428572|2017-01-09|      6.0| 3.395|      16.67|     23.5|     8.144|\n",
      "|2017-01-10|  Tuesday|   21|211.42857142857142|2017-01-10|      4.1| 1.986|       4.17|     17.2|     8.177|\n",
      "|2017-01-11|Wednesday|   23|190.43478260869566|2017-01-11|      6.4|   0.0|        0.0|     27.8|     8.211|\n",
      "|2017-01-12| Thursday|   12|             200.0|2017-01-12|      1.9|16.969|       8.33|     24.4|     8.247|\n",
      "|2017-01-13|   Friday|   10|             276.0|2017-01-13|     -1.3| 2.598|       8.33|     33.5|     8.284|\n",
      "|2017-01-14| Saturday|   11|223.63636363636363|2017-01-14|     -0.9| 0.404|       8.33|     17.6|     8.322|\n",
      "|2017-01-15|   Sunday|    5|             216.0|2017-01-15|      3.4| 5.407|       12.5|     15.9|     8.362|\n",
      "|2017-01-16|   Monday|   22|196.36363636363637|2017-01-16|      4.3| 3.409|       12.5|      8.4|     8.403|\n",
      "|2017-01-17|  Tuesday|   32|           226.875|2017-01-17|      2.7|   0.0|        0.0|      8.2|     8.445|\n",
      "|2017-01-18|Wednesday|   22|             240.0|2017-01-18|      1.1|   0.0|        0.0|     15.1|     8.488|\n",
      "|2017-01-19| Thursday|   24|             235.0|2017-01-19|      2.3|   0.0|        0.0|     12.5|     8.533|\n",
      "|2017-01-20|   Friday|   17|194.11764705882354|2017-01-20|      1.3| 0.001|       4.17|     16.7|     8.578|\n",
      "+----------+---------+-----+------------------+----------+---------+------+-----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stat = df1.groupBy(\"date\", \"DayOfWeek\")\\\n",
    "            .agg(count(\"duration\").alias(\"count\"), avg(\"duration\").alias(\"avg_duration\"))\\\n",
    "            \n",
    "            \n",
    "df_join = df_stat.join(df_weather, df1.date == df_weather.datetime, \"inner\")\n",
    "df_join.orderBy(\"date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03362b51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- avg_duration: double (nullable = true)\n",
      "\n",
      "+----------+-----+------------------+\n",
      "|      date|count|      avg_duration|\n",
      "+----------+-----+------------------+\n",
      "|2016-12-28|    2|             180.0|\n",
      "|2016-12-30|    1|             240.0|\n",
      "|2016-12-31|    1|             120.0|\n",
      "|2017-01-01|    2|             180.0|\n",
      "|2017-01-02|    7|171.42857142857142|\n",
      "|2017-01-03|   13|203.07692307692307|\n",
      "|2017-01-04|   14|188.57142857142858|\n",
      "|2017-01-05|   14|162.85714285714286|\n",
      "|2017-01-06|   10|             198.0|\n",
      "|2017-01-07|    5|             168.0|\n",
      "|2017-01-08|    2|             330.0|\n",
      "|2017-01-09|   14|205.71428571428572|\n",
      "|2017-01-10|   21|211.42857142857142|\n",
      "|2017-01-11|   23|190.43478260869566|\n",
      "|2017-01-12|   12|             200.0|\n",
      "|2017-01-13|   10|             276.0|\n",
      "|2017-01-14|   11|223.63636363636363|\n",
      "|2017-01-15|    5|             216.0|\n",
      "|2017-01-16|   22|196.36363636363637|\n",
      "|2017-01-17|   32|           226.875|\n",
      "+----------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stat.printSchema()\n",
    "df_stat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4cf8b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1084"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6b087",
   "metadata": {},
   "source": [
    "**Data transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886337bf",
   "metadata": {},
   "source": [
    "Remove weekend and some holidays (4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b058c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_join.filter(df_join.DayOfWeek != 'Sunday') \\\n",
    "                    .filter(df_join.DayOfWeek != 'Saturday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ffe9e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join = df_join.filter(df_join.date != '2017-01-01') \\\n",
    "                    .filter(df_join.date != '2017-01-02') \\\n",
    "                    .filter(df_join.date != '2017-12-25') \\\n",
    "                    .filter(df_join.date != '2017-12-26') \\\n",
    "                    .filter(df_join.date != '2018-01-01') \\\n",
    "                    .filter(df_join.date != '2018-01-02') \\\n",
    "                    .filter(df_join.date != '2018-12-25') \\\n",
    "                    .filter(df_join.date != '2018-12-26') \\\n",
    "                    .filter(df_join.date != '2019-01-01') \\\n",
    "                    .filter(df_join.date != '2019-01-02') \\\n",
    "                    .filter(df_join.date != '2019-12-25') \\\n",
    "                    .filter(df_join.date != '2019-12-26')\n",
    "df_join.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea4940e",
   "metadata": {},
   "source": [
    "*logarithm (4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "301cd294",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_join.withColumn(\"lg_day_length\", log10(col(\"day_length\"))) \\\n",
    "                .withColumn(\"lg_windspeed\", log10(col(\"windspeed\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cae81e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- avg_duration: double (nullable = true)\n",
      " |-- datetime: date (nullable = true)\n",
      " |-- feelslike: double (nullable = true)\n",
      " |-- precip: double (nullable = true)\n",
      " |-- precipcover: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- day_length: double (nullable = true)\n",
      " |-- lg_day_length: double (nullable = true)\n",
      " |-- lg_windspeed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
